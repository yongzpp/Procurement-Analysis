{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Baseline Models.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPE1YGIzq2TUYVwjokMP5QS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uNiXcCZAjmj4"},"source":["# Importing the dataframe"]},{"cell_type":"code","metadata":{"id":"Q_T1xT6KjZI6"},"source":["import os\n","os.chdir('/content/drive/My Drive/Capstone')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2C21NIPjpsW"},"source":["import pandas as pd\n","data = pd.read_excel('fiscal_y1.xlsx')\n","data = data[['Item Name', 'Item Description', 'Commodity Title']] #only interested in these 3 columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgquJtbElLzu"},"source":["# Pre-processing\n","\n","We will pre-process the data so that they can be inputs for the baseline models later.\n","\n","Steps involved here:\n","\n","    1. Clean the class labels\n","\n","    2. Removing rows which have empty Item Name and Item Description\n","\n","    3. Concatenating the Item Name and Item Description Columns so that we do not need to have 2 bags of words later.\n","\n","    4. Text Cleaning\n","\n","      - Remove Punctuation\n","      - Lowercase all the words\n","      - Tokenize the texts, meaning that each word will become a token\n","      - Removing stopwords. Stopwords are commonly used words such as 'the', 'she', 'he'.\n","      - Lemmatizing. Group words to a similar word. For example, 'good', 'better' and 'best' will be changed to 'good'.\n","      - Stemming. To convert the words to their root form. For example, 'eating' will be changed to 'eat'.\n","      - Removing special characters, such as '$', '%'\n","      - Removing numbers\n","      - Removing line breaks. They will appear as '\\r' or '\\n' in the text.\n","      - Removing single character. They are probably just typos."]},{"cell_type":"markdown","metadata":{"id":"BeDDx0bGj4FE"},"source":["Step 1: Cleaning of class labels\n","\n","We noticed that there are some rows with spacing as well as \\t and \\n in the labels. We need to clean them so that the model will not treat the label with a spacing as a different label."]},{"cell_type":"code","metadata":{"id":"ZGHjCu5akE0R"},"source":["for index, row in data.iterrows():\n","  label = str(row['Commodity Title'])\n","  if '\\t' in label:\n","    data.at[index, 'Commodity Title'] = str(row['Commodity Title']).replace('\\t', '')\n","  if '\\n' in label:\n","    data.at[index, 'Commodity Title'] = str(row['Commodity Title']).replace('\\n', '')\n","  data.at[index, 'Commodity Title'] = str(row['Commodity Title']).strip(' ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1NkUF4HmNHv"},"source":["The code below describes steps 2 and 3."]},{"cell_type":"code","metadata":{"id":"-OupDyuElKO9"},"source":["texts = []\n","classes = []\n","for index, row in data.iterrows():\n","  classname = row['Commodity Title']\n","  if len(str(row['Item Name'])) == 0 and len(str(row['Item Description'])) == 0: #whole row is empty\n","    continue\n","  if pd.isna(row['Item Description']): #keeping only 1 of the columns if the other column is empty\n","    text = str(row['Item Name'])\n","  elif pd.isna(row['Item Name']):\n","    text = str(row['Item Description'])\n","  else:\n","    text = str(row['Item Name']) + \" \" + str(row['Item Description']) #concatenate both text columns to form just 1 column\n","  if not pd.isna(classname) and not pd.isna(text): #only want rows with non-empty classes and non-empty text\n","    texts.append(text)\n","    classes.append(classname)\n","\n","df = pd.DataFrame()\n","df['Class'] = classes\n","df['Text'] = texts\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vTJW50bKmTHh"},"source":["Step 4: Text Cleaning\n","\n","We will write a function so that it is easier to automate the text cleaning later."]},{"cell_type":"code","metadata":{"id":"tR-ID9EMmUsC"},"source":["import nltk\n","import string\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","def text_cleaning(dataframe):\n","  new_df = pd.DataFrame()\n","  texts = []\n","  classes = []\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  stopwords_lst = set(stopwords.words('english'))\n","  lemmatizer = WordNetLemmatizer()\n","  stemmer = PorterStemmer()\n","  for index, row in dataframe.iterrows():\n","    text = row['Text']\n","    #remove punctuation\n","    text = ''.join([i for i in text if i not in string.punctuation])\n","    #lowercase \n","    text = ''.join([i.lower() for i in text])\n","    #tokenize\n","    text = ' '.join(tokenizer.tokenize(text))\n","    #remove stopwords\n","    text = ' '.join([i for i in text.split() if i not in stopwords_lst])\n","    #lemmatize\n","    text = ' '.join([lemmatizer.lemmatize(i) for i in text.split()])\n","    #stemming\n","    text = ' '.join([stemmer.stem(i) for i in text.split()])\n","    #remove special characters\n","    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n","    #remove numbers\n","    text = ' '.join([i for i in text.split() if i.isalpha()])\n","    #remove line breaks\n","    text = text.replace('\\n', \" \")\n","    text = text.replace('\\r', \" \")\n","    #remove single character\n","    text = ' '.join([w for w in text.split() if len(w)>1] )\n","    classes.append(row['Class'])\n","    texts.append(text)\n","  new_df['Class'] = classes\n","  new_df['Text'] = texts\n","  return new_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kjgyi2GLmZwY"},"source":["cleaned_df = text_cleaning(df)\n","\n","cleaned_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jb337fzmmiH"},"source":["Next, we will split the data into train and test so that we can see how well the models perform out of sample. But unlike the conventional train-test-split, we will be using a stratified train test split instead. We have seen from our EDA that there is a huge imbalance of classes so we want to address this problem.\n","\n","We will first separate those classes with the number of records below the threshold. They will be in the training set but won't appear in the testing set.These rows will be in the excess_df, while the bulk of the data will be in the new_df, as shown below."]},{"cell_type":"code","metadata":{"id":"X72vxQRVnJzX"},"source":["# record the number of records for each class\n","unique_classes = cleaned_df['Class'].unique()\n","unc = {}\n","for i in unique_classes:\n","  unc[i] = cleaned_df[cleaned_df['Class'] == i].shape[0]\n","pd.Series(list(unc.values())).describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTrt6uaY1dup"},"source":["# remove the classes with < 20 rows\n","texts = []\n","classes = []\n","excess_texts = []\n","excess_classes = []\n","for index, row in cleaned_df.iterrows():\n","  curr_class = row['Class']\n","  if unc[curr_class] >= 20:\n","    texts.append(row['Text'])\n","    classes.append(curr_class)\n","  else:\n","    excess_texts.append(row['Text'])\n","    excess_classes.append(curr_class)\n","\n","new_df = pd.DataFrame()\n","new_df['Class'] = classes\n","new_df['Text'] = texts\n","excess_df = pd.DataFrame()\n","excess_df['Class'] = excess_classes\n","excess_df['Text'] = excess_texts\n","print(new_df.shape)\n","print(excess_df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z76axUI9ndxg"},"source":["# vectorizing all texts with tfidf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","df = pd.concat([new_df, excess_df])\n","tfidf = TfidfVectorizer()\n","overall_mat = tfidf.fit_transform(df.Text)\n","overall_y = df.Class"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSnvcBWWnrfB"},"source":["From the overall matrix, we will split it into 2 parts. The excess matrix is for the rows in the excess df, while the main matrix is for the new df."]},{"cell_type":"code","metadata":{"id":"_03kaw7dDzGs"},"source":["excess_classes = set(list(excess_df.Class.unique())) #retrieve the list of classes that are in excess_df\n","main_idxs = []\n","excess_idxs = []\n","idx = 0\n","for i in overall_y.values: #the idea is to assign the indexes to the corresponding group based on whether the value is in the excess_classes or not.\n","  if i in excess_classes:\n","    excess_idxs.append(idx)\n","  else:\n","    main_idxs.append(idx)\n","  idx += 1\n","\n","main_mat = overall_mat[main_idxs]\n","main_y = overall_y.values[main_idxs]\n","excess_mat = overall_mat[excess_idxs]\n","excess_y = overall_y.values[excess_idxs]\n","print(main_mat.shape)\n","print(main_y.shape)\n","print(excess_mat.shape)\n","print(excess_y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHGQuaNqoiKe"},"source":["Here, we did a stratified split only on the new_df."]},{"cell_type":"code","metadata":{"id":"9xmxTocgnMYh"},"source":["# perform stratified split only on the new_df \n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 0)\n","for train_index, test_index in split.split(new_df, new_df['Class']):\n","  strat_train_set = train_index\n","  strat_test_set = test_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ozs1wA21onLJ"},"source":["Splitting the main matrix only into train and test sets. The excess matrix will be fitted into the models directly."]},{"cell_type":"code","metadata":{"id":"H9cjlACLdHWY"},"source":["# convert them into required input shape\n","corpus_train_mat = main_mat[strat_train_set,]\n","corpus_test_mat = main_mat[strat_test_set,]\n","y_train = main_y[strat_train_set]\n","y_test = main_y[strat_test_set]\n","\n","print(corpus_train_mat.shape)\n","print(corpus_test_mat.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5puwhiWkbhI"},"source":["# Baseline Models\n","\n","Now, we need to build baseline models to see how accurate the current labels are.\n","\n","Models:\n","\n","  1. Naive Bayes\n","\n","  2. Linear SVM\n","\n","  3. Logistic Regression\n","\n","  4. LSTM\n","\n","The steps for each model are the same for the first 3 models, the last model will require some more work because it is a neural network. We will fit the matrix into the model, then predict on the test set and check the evaluation metrics. We will be focusing on the f1 score and accuracy score."]},{"cell_type":"markdown","metadata":{"id":"wYyedM2PqJsN"},"source":["## Naive Bayes"]},{"cell_type":"code","metadata":{"id":"t2TgItULxlaz","executionInfo":{"status":"ok","timestamp":1602289085698,"user_tz":-480,"elapsed":18492,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"364f5d24-d3fb-4948-ba07-522e95ac8def","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.naive_bayes import MultinomialNB\n","nbclassifier = MultinomialNB()\n","nbclassifier.fit(excess_mat, excess_y)\n","nbclassifier.fit(corpus_train_mat, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Iu7AmxEs0bhp"},"source":["#predict class from test data \n","predicted = nbclassifier.predict(corpus_test_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JJgLbBM3eR8","executionInfo":{"status":"ok","timestamp":1602289087239,"user_tz":-480,"elapsed":1533,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"987ff41c-7175-4434-91ff-d2b688ef9643","colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print(\"Accuracy: \" + str(accuracy_score(y_test, predicted)))\n","print(\"Precision: \" + str(precision_score(y_test, predicted, average = 'weighted')))\n","print(\"Recall: \" + str(recall_score(y_test, predicted, average = 'weighted')))\n","print(\"F1 Score: \" + str(f1_score(y_test, predicted, average = 'weighted')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.40567294879116694\n","Precision: 0.3940133815195956\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Recall: 0.40567294879116694\n","F1 Score: 0.33955390390274726\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S66mzLfjqx3t"},"source":["## Linear SVM"]},{"cell_type":"code","metadata":{"id":"uL6mDCjdCP9q"},"source":["from sklearn.svm import SVC\n","\n","text_clf = SVC(kernel='linear', probability = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aloik3TlCP9z"},"source":["#train model\n","text_clf.fit(excess_mat, excess_y)\n","text_clf.fit(corpus_train_mat, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bup1iV-JCP99"},"source":["#predict class from test data \n","predicted = text_clf.predict(corpus_test_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u40EQwGhAwyf","executionInfo":{"status":"ok","timestamp":1602296836833,"user_tz":-480,"elapsed":4553428,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"2e7f3f92-59f7-45bd-d122-a428fc213294","colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print(\"Accuracy: \" + str(accuracy_score(y_test, predicted)))\n","print(\"Precision: \" + str(precision_score(y_test, predicted, average = 'weighted')))\n","print(\"Recall: \" + str(recall_score(y_test, predicted, average = 'weighted')))\n","print(\"F1 Score: \" + str(f1_score(y_test, predicted, average = 'weighted')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.6548638873024938\n","Precision: 0.6686171504585076\n","Recall: 0.6548638873024938\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["F1 Score: 0.6377390457188284\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j_OM-_hSq_6a"},"source":["## Logistic Regression"]},{"cell_type":"code","metadata":{"id":"n7w3YfLkrIIq","executionInfo":{"status":"ok","timestamp":1602241391986,"user_tz":-480,"elapsed":368274,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"acf89fda-9d45-4064-f5e4-61e443deade3","colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression(random_state=0, solver='sag')\n","clf.fit(excess_mat, excess_y)\n","clf.fit(corpus_train_mat, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=0, solver='sag', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"RYFGtevYsp7J"},"source":["#predict class from test data \n","predicted = clf.predict(corpus_test_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6s3FSe9sudU","executionInfo":{"status":"ok","timestamp":1599884443865,"user_tz":-480,"elapsed":1158,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"f48558c7-2ee1-4b6d-deb6-156442599ccd","colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print(\"Accuracy: \" + str(accuracy_score(y_test, predicted)))\n","print(\"Precision: \" + str(precision_score(y_test, predicted, average = 'weighted')))\n","print(\"Recall: \" + str(recall_score(y_test, predicted, average = 'weighted')))\n","print(\"F1 Score: \" + str(f1_score(y_test, predicted, average = 'weighted')))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.6170077133932556\n","Precision: 0.6190028626563597\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Recall: 0.6170077133932556\n","F1 Score: 0.5903056618496567\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2pypJKoVrKur"},"source":["## LSTM\n","\n","This model requires more pre-processing work. We need to set the X sequences to a fixed length so the input matrices will be different from the input matrices used for the earlier models."]},{"cell_type":"markdown","metadata":{"id":"ojehTIdnSBwu"},"source":["### Pre-processing"]},{"cell_type":"code","metadata":{"id":"YVUeQWTY1hh3"},"source":["from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.models import Sequential"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsETZcQOsXUf"},"source":["Prepare the X and Y"]},{"cell_type":"code","metadata":{"id":"3BNDHxYFFWqG"},"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","\n","# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 50000\n","# Max number of words in each row.\n","MAX_SEQUENCE_LENGTH = 250\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(cleaned_df['Text'].values)\n","word_index = tokenizer.word_index #this is important for evaluation of final test dataset\n","\n","X = tokenizer.texts_to_sequences(cleaned_df['Text'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvQmOolJK2m7"},"source":["Y = pd.get_dummies(cleaned_df['Class']).values\n","print('Shape of label tensor:', Y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSo-QseRscR_"},"source":["Use the main and excess indexes we have gotten earlier to split X and Y into the main inputs and the excess inputs"]},{"cell_type":"code","metadata":{"id":"Enjetvv_5y-i"},"source":["X_main = X[main_idxs]\n","X_excess = X[excess_idxs]\n","Y_main = Y[main_idxs]\n","Y_excess = Y[excess_idxs]\n","print(X_main.shape)\n","print(X_excess.shape)\n","print(Y_main.shape)\n","print(Y_excess.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-D79zeasp5W"},"source":["Like before, only X_main will be split into train and test set"]},{"cell_type":"code","metadata":{"id":"MbdzQg-QK9zK"},"source":["corpus_train_mat = X_main[strat_train_set,] #can still use back the same indices from the earlier stratified train test split\n","corpus_test_mat = X_main[strat_test_set,]\n","y_train = Y_main[strat_train_set]\n","y_test = Y_main[strat_test_set]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2d_uubgnsx5M"},"source":["### Build LSTM Model"]},{"cell_type":"code","metadata":{"id":"StljFcANGBAV","executionInfo":{"status":"ok","timestamp":1602371710148,"user_tz":-480,"elapsed":6305,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"32fd3ea9-3d04-4873-f5fc-f70825107f32","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(Y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PLZhUJHws7G1"},"source":["Early stop the training if the validation loss did not improve in 10 epochs. We will also save the current training model if it has a lower validation loss than the previous saved one. "]},{"cell_type":"code","metadata":{"id":"oIorkuZu28E_"},"source":["es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n","cp = ModelCheckpoint('before_labels_lstm_model_threshold_20.hdf5',monitor='val_loss', save_best_only = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5h2EaOVctL4O"},"source":["So for the training, we will first fit the excess data (below threshold) to the model first and let the model train on it for 10 epochs. Then, we will fit it to the bulk of the data with the test set."]},{"cell_type":"code","metadata":{"id":"lWjm_Wp-5MWa"},"source":["#excess data train for 10 epochs\n","\n","model.fit(X_excess, Y_excess, epochs = 10, batch_size = 32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95zU2YQitgIJ"},"source":["The validation accuracy is 61%."]},{"cell_type":"code","metadata":{"id":"_L14FF05cRmo"},"source":["epochs = 50\n","batch_size = 32\n","\n","history = model.fit(corpus_train_mat, y_train, epochs=epochs, validation_data=(corpus_test_mat, y_test),\n","                    batch_size=batch_size,callbacks=[cp, es])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDzbGCbWSK-S"},"source":["### Get F1 Score"]},{"cell_type":"markdown","metadata":{"id":"cg3QyDaitrA_"},"source":["Now, we need to get the F1 score. We can't get the F1 score from model training as the f1 score metric is not supported by keras. Hence, we need to get the model to predict on the test set again and calculate the f1 score from there. First, we need to load the best saved LSTM model."]},{"cell_type":"code","metadata":{"id":"mt-Q-CmFmtLl","executionInfo":{"status":"ok","timestamp":1602922547361,"user_tz":-480,"elapsed":10239,"user":{"displayName":"Victor Cheong","photoUrl":"","userId":"12873658511139227671"}},"outputId":"38e46b1a-8e04-4d1f-e3b2-3c38ac5271c3","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from keras.models import load_model\n","\n","# returns a compiled model identical to the previous one\n","model = load_model('before_labels_lstm_model_threshold_20.hdf5')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LqT-OKdTuJQf"},"source":["Get predictions for test set"]},{"cell_type":"code","metadata":{"id":"K3N9C5AxXnwy"},"source":["predictions = model.predict(corpus_test_mat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oiipf2zmuO5g"},"source":["The model will return a list of probabilities for each row where each probability corresponds to the probability of that row belonging to this class. So we will take the class with the highest probability as the predicted class for that row. "]},{"cell_type":"code","metadata":{"id":"vDWO635gZSad"},"source":["import numpy as np\n","\n","predictions_ans = []\n","for i in predictions:\n","  predictions_ans.append(np.argmax(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNfTVMxKuj6R"},"source":["Since we encoded the Y by dummy variables earlier, each row will only have a 1 and the rest are 0. The 1 for that column will tell us the correct class for this row."]},{"cell_type":"code","metadata":{"id":"h9MwpyxTYj14"},"source":["correct_ans = []\n","for i in y_test:\n","  correct_ans.append(i.tolist().index(1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mdfwkgzux-7"},"source":["Retrieving the F1 score. It is found to be 58%."]},{"cell_type":"code","metadata":{"id":"mmRI5RgRbZK7"},"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","print(\"Accuracy: \" + str(accuracy_score(correct_ans, predictions_ans)))\n","print(\"Precision: \" + str(precision_score(correct_ans, predictions_ans, average = 'weighted')))\n","print(\"Recall: \" + str(recall_score(correct_ans, predictions_ans, average = 'weighted')))\n","print(\"F1 Score: \" + str(f1_score(correct_ans, predictions_ans, average = 'weighted')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jh9_i-qWvC52"},"source":["As a summary, here are the accuracies and F1 scores we obtained for our baseline models. \n","\n","1. Naive Bayes: accuracy- 40, f1 score- 33\n","\n","2. Linear SVM: accuracy- 65, f1 score- 64\n","\n","3. Logistic Regression: accuracy- 61, f1 score- 59\n","\n","4. LSTM: accuracy- 61, f1 score- 58\n"]}]}